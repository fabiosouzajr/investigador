{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dafe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting chromedriver_autoinstaller\n",
      "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.12/dist-packages (from chromedriver_autoinstaller) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
      "Downloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, outcome, chromedriver_autoinstaller, trio, trio-websocket, selenium\n",
      "Successfully installed chromedriver_autoinstaller-0.6.4 outcome-1.3.0.post0 selenium-4.38.0 trio-0.32.0 trio-websocket-0.12.2 wsproto-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install selenium chromedriver_autoinstaller requests beautifulsoup4 tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c33d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1208090898.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Mount drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set base directory in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (if not already mounted)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Check if drive is already mounted\n",
    "drive_mounted = os.path.exists('/content/drive/MyDrive')\n",
    "\n",
    "if not drive_mounted:\n",
    "    try:\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        print(\"You will need to authenticate. Follow the instructions in the output below.\")\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        drive_mounted = True\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not mount Google Drive: {e}\")\n",
    "        print(\"This might happen if:\")\n",
    "        print(\"  1. Drive is already mounted (this is OK)\")\n",
    "        print(\"  2. Authentication timed out (try running this cell again)\")\n",
    "        print(\"  3. There was a connection issue\")\n",
    "        # Check again after the error\n",
    "        drive_mounted = os.path.exists('/content/drive/MyDrive')\n",
    "        if not drive_mounted:\n",
    "            print(\"\\nUsing local storage instead. Files will be saved to /content/diarios_ceara\")\n",
    "            print(\"Note: Files in /content will be lost when the Colab session ends.\")\n",
    "            BASE_DIR = \"/content/diarios_ceara\"\n",
    "        else:\n",
    "            print(\"Drive appears to be mounted now.\")\n",
    "            BASE_DIR = \"/content/drive/MyDrive/diarios_ceara\"\n",
    "else:\n",
    "    print(\"Google Drive is already mounted.\")\n",
    "    BASE_DIR = \"/content/drive/MyDrive/diarios_ceara\"\n",
    "\n",
    "# Create base directory\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "print(f\"\\nFiles will be saved to: {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e234ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "import re\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "# Setup ChromeDriver for Colab\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "BASE_URL_SHORTCUT = \"http://pesquisa.doe.seplag.ce.gov.br/doepesquisa/sead.do?page=ultimasDetalhe&cmd=10&action=Cadernos&data=\"\n",
    "BASE_URL_MAIN = \"http://pesquisa.doe.seplag.ce.gov.br/doepesquisa/sead.do?page=ultimasEdicoes&cmd=11&action=Ultimas\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def extract_filename_from_url(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract filename from URL by checking query parameters, path, and Content-Disposition.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to extract filename from\n",
    "        \n",
    "    Returns:\n",
    "        Filename if found, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Check query parameters for common filename parameters\n",
    "        query_params = parse_qs(parsed.query)\n",
    "        for param in ['arquivo', 'file', 'filename', 'nome', 'documento']:\n",
    "            if param in query_params:\n",
    "                filename = query_params[param][0]\n",
    "                if filename and filename.lower().endswith('.pdf'):\n",
    "                    return unquote(filename)\n",
    "        \n",
    "        # Check path for PDF filename\n",
    "        path = parsed.path\n",
    "        if path:\n",
    "            # Look for .pdf in the path\n",
    "            pdf_match = re.search(r'([^/]+\\.pdf)', path, re.IGNORECASE)\n",
    "            if pdf_match:\n",
    "                return unquote(pdf_match.group(1))\n",
    "            \n",
    "            # Get last part of path\n",
    "            basename = os.path.basename(path)\n",
    "            if basename and basename.lower().endswith('.pdf'):\n",
    "                return unquote(basename)\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error extracting filename from URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_filename_from_content_disposition(url: str, headers: dict) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try to get filename from Content-Disposition header by making a HEAD request.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to check\n",
    "        headers: Headers to use for request\n",
    "        \n",
    "    Returns:\n",
    "        Filename if found in Content-Disposition header, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "        content_disposition = response.headers.get('Content-Disposition', '')\n",
    "        if content_disposition:\n",
    "            # Parse Content-Disposition: attachment; filename=\"file.pdf\"\n",
    "            filename_match = re.search(r'filename[^;=\\n]*=(([\\'\"]).*?\\2|[^;\\n]*)', content_disposition, re.IGNORECASE)\n",
    "            if filename_match:\n",
    "                filename = filename_match.group(1).strip('\"\\'')\n",
    "                return unquote(filename)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_dates(mode: str, value: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate list of dates in YYYYMMDD format.\n",
    "    \n",
    "    Args:\n",
    "        mode: 'year' or 'days'\n",
    "        value: Year number or number of days\n",
    "        \n",
    "    Returns:\n",
    "        List of date strings in YYYYMMDD format\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    \n",
    "    if mode == 'year':\n",
    "        # Generate all dates in the year\n",
    "        start_date = datetime(value, 1, 1)\n",
    "        # Check if year is current year\n",
    "        if value == datetime.now().year:\n",
    "            end_date = datetime.now()\n",
    "        else:\n",
    "            end_date = datetime(value, 12, 31)\n",
    "        \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    \n",
    "    elif mode == 'days':\n",
    "        # Generate last X days from today\n",
    "        today = datetime.now()\n",
    "        for i in range(value):\n",
    "            date = today - timedelta(days=i)\n",
    "            dates.append(date.strftime('%Y%m%d'))\n",
    "    \n",
    "    return dates\n",
    "\n",
    "\n",
    "def setup_directories(base_path: str, years: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Create directory structure for storing downloaded files.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base directory path\n",
    "        years: List of years to create subdirectories for\n",
    "    \"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    for year in years:\n",
    "        year_path = os.path.join(base_path, str(year))\n",
    "        os.makedirs(year_path, exist_ok=True)\n",
    "        logger.info(f\"Created directory: {year_path}\")\n",
    "\n",
    "print(\"Helper functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0924f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping functions\n",
    "def test_shortcut_url(date_str: str) -> Tuple[bool, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Test if shortcut URL works for a given date.\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date in YYYYMMDD format\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (success: bool, links: List[Tuple[filename, url]])\n",
    "    \"\"\"\n",
    "    url = BASE_URL_SHORTCUT + date_str\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for table with PDF links\n",
    "        links = []\n",
    "        \n",
    "        # Try to find links in tables\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                for cell in cells:\n",
    "                    link = cell.find('a', href=True)\n",
    "                    if not link:\n",
    "                        continue\n",
    "                    \n",
    "                    href = link.get('href', '')\n",
    "                    if not ('.pdf' in href.lower() or 'download' in href.lower() or 'baixar' in href.lower()):\n",
    "                        continue\n",
    "                    \n",
    "                    # Make absolute URL if relative\n",
    "                    if href.startswith('/'):\n",
    "                        full_url = f\"http://pesquisa.doe.seplag.ce.gov.br{href}\"\n",
    "                    elif href.startswith('http'):\n",
    "                        full_url = href\n",
    "                    else:\n",
    "                        full_url = f\"http://pesquisa.doe.seplag.ce.gov.br/doepesquisa/{href}\"\n",
    "                    \n",
    "                    # Try multiple methods to get filename\n",
    "                    filename = None\n",
    "                    \n",
    "                    # Method 1: Extract from URL\n",
    "                    filename = extract_filename_from_url(full_url)\n",
    "                    \n",
    "                    # Method 2: Look in table cells\n",
    "                    if not filename or filename.lower() in ['visualizar.pdf', 'visualizar', 'baixar.pdf', 'download.pdf']:\n",
    "                        for other_cell in cells:\n",
    "                            cell_text = other_cell.get_text(strip=True)\n",
    "                            if (cell_text and \n",
    "                                cell_text.lower() not in ['visualizar', 'baixar', 'download', 'ver'] and\n",
    "                                (cell_text.lower().endswith('.pdf') or len(cell_text) > 5)):\n",
    "                                filename = cell_text\n",
    "                                if not filename.endswith('.pdf'):\n",
    "                                    filename += '.pdf'\n",
    "                                break\n",
    "                    \n",
    "                    # Method 3: Try to get from Content-Disposition header\n",
    "                    if not filename or filename.lower() in ['visualizar.pdf', 'visualizar', 'baixar.pdf', 'download.pdf']:\n",
    "                        filename = get_filename_from_content_disposition(full_url, headers)\n",
    "                    \n",
    "                    # Method 4: Use link text if it's not a generic action word\n",
    "                    if not filename or filename.lower() in ['visualizar.pdf', 'visualizar', 'baixar.pdf', 'download.pdf']:\n",
    "                        link_text = link.get_text(strip=True)\n",
    "                        if link_text and link_text.lower() not in ['visualizar', 'baixar', 'download', 'ver']:\n",
    "                            filename = link_text\n",
    "                        else:\n",
    "                            # Fallback: use basename from URL\n",
    "                            filename = os.path.basename(urlparse(full_url).path) or 'documento.pdf'\n",
    "                    \n",
    "                    # Ensure .pdf extension\n",
    "                    if not filename.endswith('.pdf'):\n",
    "                        filename += '.pdf'\n",
    "                    \n",
    "                    # Clean filename\n",
    "                    filename = unquote(filename)\n",
    "                    \n",
    "                    links.append((filename, full_url))\n",
    "        \n",
    "        # Also check for direct PDF links in the page\n",
    "        if not links:\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href', '')\n",
    "                if href.lower().endswith('.pdf'):\n",
    "                    if href.startswith('/'):\n",
    "                        full_url = f\"http://pesquisa.doe.seplag.ce.gov.br{href}\"\n",
    "                    elif href.startswith('http'):\n",
    "                        full_url = href\n",
    "                    else:\n",
    "                        full_url = f\"http://pesquisa.doe.seplag.ce.gov.br/doepesquisa/{href}\"\n",
    "                    \n",
    "                    # Try to extract filename from URL\n",
    "                    filename = extract_filename_from_url(full_url)\n",
    "                    if not filename:\n",
    "                        filename = os.path.basename(urlparse(full_url).path) or 'documento.pdf'\n",
    "                    filename = unquote(filename)\n",
    "                    if not filename.endswith('.pdf'):\n",
    "                        filename += '.pdf'\n",
    "                    links.append((filename, full_url))\n",
    "        \n",
    "        # Check if page indicates no publications\n",
    "        page_text = soup.get_text().lower()\n",
    "        if any(phrase in page_text for phrase in ['não há', 'sem dados', 'nenhuma publicação', 'não existem']):\n",
    "            return True, []  # Page loaded but no publications\n",
    "        \n",
    "        return True, links\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error testing shortcut URL for {date_str}: {e}\")\n",
    "        return False, []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error testing shortcut URL for {date_str}: {e}\")\n",
    "        return False, []\n",
    "\n",
    "\n",
    "def extract_pdf_links_shortcut(date_str: str) -> Tuple[List[Tuple[str, str]], bool]:\n",
    "    \"\"\"\n",
    "    Extract PDF links using shortcut URL method.\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date in YYYYMMDD format\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (links: List[Tuple[filename, url]], shortcut_worked: bool)\n",
    "    \"\"\"\n",
    "    success, links = test_shortcut_url(date_str)\n",
    "    return (links, success)\n",
    "\n",
    "\n",
    "def extract_pdf_links_selenium(year: int, date_str: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract PDF links using Selenium navigation (fallback method).\n",
    "    \n",
    "    Args:\n",
    "        year: Year as integer\n",
    "        date_str: Date in YYYYMMDD format\n",
    "        \n",
    "    Returns:\n",
    "        List of (filename, url) tuples\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        # Setup Chrome options for Colab\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument(f'user-agent={USER_AGENT}')\n",
    "        \n",
    "        # Initialize driver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        # Navigate to main page\n",
    "        logger.info(f\"Navigating to main page for Selenium fallback\")\n",
    "        driver.get(BASE_URL_MAIN)\n",
    "        \n",
    "        # Wait for and select year dropdown\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        year_select_element = wait.until(\n",
    "            EC.presence_of_element_located((By.NAME, \"DiarioGrid\"))\n",
    "        )\n",
    "        year_select = Select(year_select_element)\n",
    "        year_select.select_by_value(str(year))\n",
    "        \n",
    "        # Wait for date dropdown to appear\n",
    "        date_select_element = wait.until(\n",
    "            EC.presence_of_element_located((By.NAME, \"DiarioAjaxGridBaixar\"))\n",
    "        )\n",
    "        date_select = Select(date_select_element)\n",
    "        \n",
    "        # Format date for selection\n",
    "        date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "        date_formats = [\n",
    "            date_obj.strftime('%d/%m/%Y'),\n",
    "            date_obj.strftime('%Y-%m-%d'),\n",
    "            date_str\n",
    "        ]\n",
    "        \n",
    "        date_selected = False\n",
    "        for date_format in date_formats:\n",
    "            try:\n",
    "                date_select.select_by_value(date_format)\n",
    "                date_selected = True\n",
    "                break\n",
    "            except:\n",
    "                try:\n",
    "                    date_select.select_by_visible_text(date_format)\n",
    "                    date_selected = True\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if not date_selected:\n",
    "            logger.warning(f\"Could not select date {date_str} in dropdown\")\n",
    "            return []\n",
    "        \n",
    "        # Wait for popup/table to appear\n",
    "        try:\n",
    "            table = wait.until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            logger.warning(f\"No table found for date {date_str}\")\n",
    "            return []\n",
    "        \n",
    "        # Extract links from table\n",
    "        links = []\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            for cell in cells:\n",
    "                try:\n",
    "                    link_elem = cell.find_element(By.TAG_NAME, \"a\")\n",
    "                    href = link_elem.get_attribute('href')\n",
    "                    if not href or not ('.pdf' in href.lower() or 'download' in href.lower() or 'baixar' in href.lower()):\n",
    "                        continue\n",
    "                    \n",
    "                    # Try multiple methods to get filename\n",
    "                    filename = None\n",
    "                    \n",
    "                    # Method 1: Extract from URL\n",
    "                    filename = extract_filename_from_url(href)\n",
    "                    \n",
    "                    # Method 2: Look in table cells\n",
    "                    if not filename or filename.lower() in ['visualizar.pdf', 'visualizar', 'baixar.pdf', 'download.pdf']:\n",
    "                        for other_cell in cells:\n",
    "                            try:\n",
    "                                cell_text = other_cell.text.strip()\n",
    "                                if (cell_text and \n",
    "                                    cell_text.lower() not in ['visualizar', 'baixar', 'download', 'ver'] and\n",
    "                                    (cell_text.lower().endswith('.pdf') or len(cell_text) > 5)):\n",
    "                                    filename = cell_text\n",
    "                                    if not filename.endswith('.pdf'):\n",
    "                                        filename += '.pdf'\n",
    "                                    break\n",
    "                            except:\n",
    "                                continue\n",
    "                    \n",
    "                    # Method 3: Try to get from Content-Disposition header\n",
    "                    if not filename or filename.lower() in ['visualizar.pdf', 'visualizar', 'baixar.pdf', 'download.pdf']:\n",
    "                        filename = get_filename_from_content_disposition(href, {'User-Agent': USER_AGENT})\n",
    "                    \n",
    "                    # Method 4: Use link text if it's not a generic action word\n",
    "                    if not filename or filename.lower() in ['visualizar.pdf', 'visualizar', 'baixar.pdf', 'download.pdf']:\n",
    "                        link_text = link_elem.text.strip()\n",
    "                        if link_text and link_text.lower() not in ['visualizar', 'baixar', 'download', 'ver']:\n",
    "                            filename = link_text\n",
    "                        else:\n",
    "                            # Fallback: use basename from URL\n",
    "                            filename = os.path.basename(urlparse(href).path) or 'documento.pdf'\n",
    "                    \n",
    "                    # Ensure .pdf extension\n",
    "                    if not filename.endswith('.pdf'):\n",
    "                        filename += '.pdf'\n",
    "                    \n",
    "                    # Clean filename\n",
    "                    filename = unquote(filename)\n",
    "                    \n",
    "                    links.append((filename, href))\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error extracting link from cell: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return links\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Selenium extraction for {date_str}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "def download_file(url: str, filepath: str, max_retries: int = 3) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Download file with progress bar and retry logic.\n",
    "    Automatically overwrites existing files.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to download\n",
    "        filepath: Local file path to save to\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (success: bool, status: str) where status is 'downloaded', 'skipped', or 'failed'\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Try to get actual filename from Content-Disposition header\n",
    "            content_disposition = response.headers.get('Content-Disposition', '')\n",
    "            actual_filename = None\n",
    "            if content_disposition:\n",
    "                filename_match = re.search(r'filename[^;=\\n]*=(([\\'\"]).*?\\2|[^;\\n]*)', content_disposition, re.IGNORECASE)\n",
    "                if filename_match:\n",
    "                    actual_filename = filename_match.group(1).strip('\"\\'')\n",
    "                    actual_filename = unquote(actual_filename)\n",
    "            \n",
    "            # If we got a better filename from headers, update filepath\n",
    "            if actual_filename and actual_filename.lower().endswith('.pdf'):\n",
    "                # Only update if current filename is generic\n",
    "                current_basename = os.path.basename(filepath).lower()\n",
    "                if current_basename in ['visualizar.pdf', 'baixar.pdf', 'download.pdf', 'documento.pdf']:\n",
    "                    # Update filepath with actual filename\n",
    "                    dir_path = os.path.dirname(filepath)\n",
    "                    filepath = os.path.join(dir_path, actual_filename)\n",
    "                    logger.info(f\"Using filename from Content-Disposition: {actual_filename}\")\n",
    "            \n",
    "            # Get file size from headers\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Get filename for display\n",
    "            filename = os.path.basename(filepath)\n",
    "            \n",
    "            # Automatically overwrite existing files (no prompt)\n",
    "            if os.path.exists(filepath):\n",
    "                logger.info(f\"Overwriting existing file: {filename}\")\n",
    "            \n",
    "            # Download with progress bar\n",
    "            with open(filepath, 'wb') as f:\n",
    "                if total_size > 0:\n",
    "                    with tqdm(\n",
    "                        total=total_size,\n",
    "                        unit='B',\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                        desc=filename,\n",
    "                        ncols=100\n",
    "                    ) as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "                else:\n",
    "                    # Unknown size, still show progress\n",
    "                    with tqdm(\n",
    "                        unit='B',\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                        desc=filename,\n",
    "                        ncols=100\n",
    "                    ) as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "            \n",
    "            logger.info(f\"Successfully downloaded: {filename}\")\n",
    "            return (True, 'downloaded')\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {e}. Retrying...\")\n",
    "            else:\n",
    "                logger.error(f\"Failed to download {url} after {max_retries} attempts: {e}\")\n",
    "                return (False, 'failed')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error downloading {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"Retrying...\")\n",
    "            else:\n",
    "                return (False, 'failed')\n",
    "    \n",
    "    return (False, 'failed')\n",
    "\n",
    "print(\"Scraping functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution with interactive prompts\n",
    "def get_user_input() -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Get user input for download mode and value.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (mode: 'year' or 'days', value: int)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"Download by (1) Year or (2) Last X days? Enter 1 or 2: \").strip()\n",
    "            if choice == '1':\n",
    "                year = input(\"Enter year (e.g., 2024): \").strip()\n",
    "                year_int = int(year)\n",
    "                if 2000 <= year_int <= datetime.now().year + 1:\n",
    "                    return ('year', year_int)\n",
    "                else:\n",
    "                    print(f\"Please enter a valid year between 2000 and {datetime.now().year + 1}\")\n",
    "            elif choice == '2':\n",
    "                days = input(\"Enter number of days: \").strip()\n",
    "                days_int = int(days)\n",
    "                if days_int > 0:\n",
    "                    return ('days', days_int)\n",
    "                else:\n",
    "                    print(\"Please enter a positive number of days\")\n",
    "            else:\n",
    "                print(\"Please enter 1 or 2\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperation cancelled by user\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "# Main execution\n",
    "logger.info(\"Starting Diários Oficiais Ceará Scraper\")\n",
    "\n",
    "# Get user input\n",
    "mode, value = get_user_input()\n",
    "if mode is None or value is None:\n",
    "    print(\"Operation cancelled.\")\n",
    "else:\n",
    "    logger.info(f\"Mode: {mode}, Value: {value}\")\n",
    "    \n",
    "    # Generate dates\n",
    "    dates = generate_dates(mode, value)\n",
    "    logger.info(f\"Generated {len(dates)} dates to process\")\n",
    "    \n",
    "    # Extract unique years from dates\n",
    "    years = sorted(set(int(date[:4]) for date in dates))\n",
    "    \n",
    "    # Setup directories\n",
    "    setup_directories(BASE_DIR, years)\n",
    "    \n",
    "    # Statistics\n",
    "    total_downloaded = 0\n",
    "    total_skipped = 0\n",
    "    total_failed = 0\n",
    "    dates_with_publications = 0\n",
    "    dates_without_publications = 0\n",
    "    \n",
    "    # Process each date\n",
    "    for date_str in dates:\n",
    "        year = int(date_str[:4])\n",
    "        logger.info(f\"Processing date: {date_str}\")\n",
    "        \n",
    "        # Try shortcut method first\n",
    "        links, shortcut_worked = extract_pdf_links_shortcut(date_str)\n",
    "        \n",
    "        # If shortcut failed to load page, try Selenium\n",
    "        # If shortcut worked but returned no links, it means no publications (don't try Selenium)\n",
    "        if not shortcut_worked:\n",
    "            logger.info(f\"Shortcut method failed for {date_str}, trying Selenium fallback\")\n",
    "            links = extract_pdf_links_selenium(year, date_str)\n",
    "        \n",
    "        if not links:\n",
    "            logger.info(f\"No publications found for date {date_str}\")\n",
    "            dates_without_publications += 1\n",
    "            continue\n",
    "        \n",
    "        dates_with_publications += 1\n",
    "        logger.info(f\"Found {len(links)} files for date {date_str}\")\n",
    "        \n",
    "        # Download each file\n",
    "        for filename, url in links:\n",
    "            # Clean filename (remove invalid characters)\n",
    "            safe_filename = \"\".join(c for c in filename if c.isalnum() or c in \"._- \")\n",
    "            if not safe_filename.endswith('.pdf'):\n",
    "                safe_filename += '.pdf'\n",
    "            \n",
    "            filepath = os.path.join(BASE_DIR, str(year), safe_filename)\n",
    "            \n",
    "            success, status = download_file(url, filepath)\n",
    "            if status == 'downloaded':\n",
    "                total_downloaded += 1\n",
    "            elif status == 'skipped':\n",
    "                total_skipped += 1\n",
    "            elif status == 'failed':\n",
    "                total_failed += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DOWNLOAD SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total dates processed: {len(dates)}\")\n",
    "    print(f\"Dates with publications: {dates_with_publications}\")\n",
    "    print(f\"Dates without publications: {dates_without_publications}\")\n",
    "    print(f\"Files downloaded: {total_downloaded}\")\n",
    "    print(f\"Files skipped: {total_skipped}\")\n",
    "    print(f\"Files failed: {total_failed}\")\n",
    "    print(\"=\"*60)\n",
    "    logger.info(\"Scraper finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
